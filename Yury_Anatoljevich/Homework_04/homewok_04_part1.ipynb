{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание:\n",
    "1.\tЗагрузить файл длиной не менее 2000 символов. \n",
    "2.\tСоставить программу, которая считает число уникальных слов в тексте (без критерия схожести)\n",
    "3.\tСоставить программу, которая считает число гласных и согласных букв. \n",
    "4.\tСоставить программу, которая считает число предложений, их длину и число (количество) раз использования каждого слова в тексте (с критерием схожести, критерий схожести слов выбрать самостоятельно, например, spacy (en_core_web_sm) или расстояние Левенштейна). \n",
    "5.\tВывести 10 наиболее часто встречаемых слов. \n",
    "\n",
    "***\n",
    "Все задание делится на несколько частей. Соответственно, для каждой части создаем свой jupiter notebook.\n",
    "В данном блокноте выполняем пункт 2 задания.\n",
    "\n",
    "***\n",
    "Анализ по пункту 2 задания (с расширением по пункту 5 -  будет интересно сравнить, различается ли первая десятка, если уникальные слова отбираются без критерия схожести и с использованием критерия схожести):\n",
    "\n",
    "Для подсчета уникальных слов в тексте без критерия схожести можно находить каждое следующее слово и заносить его в словарь dict в виде ключа, если этого ключа еще нет в словаре. Значение value, соответствующее этому ключу, будет присваиваться равным единице. Если же такой ключ уже есть, то соответствующее ему значение value увеличивается на единицу. После прохождения всего текста числом уникальных слов будет считаться число ключей в полученном словаре. \n",
    "Поскольку наиболее частотными будут не значащие стоп-слова типа артиклей, предлогов, местоимений, следует их предварительно удалить из текста.\n",
    "Значения ключей будут далее использованы для нахождения 10 наиболее встречаемых слов. Для этого нужно отсортировать словарь по значениям полей values и взять те десять ключей, которым будут соответствует максимальное значение values\n",
    "Если выяснится, что десятое по встречаемости слово встречается n раз, и имеется еще несколько слов, не включенных в десятку, но которые тоже встречаются n раз, то придется расширить десятку, включив в нее и эти слова. \n",
    "\n",
    "Чтобы процессу не мешали числа и знаки препинания, можно предварительно заместить их все пробелами (для этого будем с помощью встроенной функции Python проверять, относится ли каждый знак к алфавитным). \n",
    "Чтобы не было путаницы с регистрами текста, предварительно приводим весь текст к нижнему регистру.\n",
    "\n",
    "Тогда план работы по пункту 2 следующий.\n",
    "\n",
    "1. Загрузить текст из файла.\n",
    "2. Перевести текст в нижний регистр.\n",
    "3. Убрать знаки препинания и шире - все неалфавитные знаки, заменив их пробелами.\n",
    "4. Разбить текст на слова, считая пробелы разделителями.\n",
    "5. Из полученного списка слов убрать все стоп-слова.\n",
    "6. Занести в словарь все слова из списка, для каждого указать число раз, сколько оно встречалось в списке.\n",
    "7. Отсортировать полученный словарь по значению ключа (дающего частотность соответствующего слова)\n",
    "8. Отобрать 10 наиболее часто встречающихся слов в полученном словаре.\n",
    "9. Если в полученном словаре есть еще слова с той же частотностью, что и у десятого слова, также включить их в выводимый результат."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем текстовый файл:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read successfully\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "\n",
    "file_name = 'GospelJohn.txt' # file with text\n",
    "try:\n",
    "    f = open(file_name,\"r\") # open file for reading\n",
    "    text = f.read()         # reading file \n",
    "    f.close()               # closing file\n",
    "    print('Read successfully')\n",
    "except:\n",
    "    print('Error reading file!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Переводим текст в нижний регистр.\n",
    "Убираем не-алфавитные знаки и разбиваем текст на слова, помещаем их в список."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lower case:\n",
    "text = text.lower()\n",
    "# replace non-alphabetic signs by whitespaces:\n",
    "text_no_signs = \"\".join([c if c.isalpha() else \" \" for c in text])\n",
    "\n",
    "word_list = text_no_signs.split() # split text into words and put to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем из списка лишние слова (\"стоп-слова\").\n",
    "Для этого вначале создаем список стоп-слов\n",
    "(источник: https://github.com/explosion/spaCy/blob/master/spacy/lang/en/stop_words.py)\n",
    "Также мы немного расширяем этот список для учета устаревших слов, фигурирующих в нашем тексте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = set(\n",
    "    \"\"\"\n",
    "a about above across after afterwards again against all almost alone along\n",
    "already also although always am among amongst amount an and another any anyhow\n",
    "anyone anything anyway anywhere are around as at\n",
    "\n",
    "back be became because become becomes becoming been before beforehand behind\n",
    "being below beside besides between beyond both bottom but by\n",
    "\n",
    "call can cannot ca could\n",
    "\n",
    "did do does doing done down due during\n",
    "\n",
    "each eight either eleven else elsewhere empty enough even ever every\n",
    "everyone everything everywhere except\n",
    "\n",
    "few fifteen fifty first five for former formerly forty four from front full\n",
    "further\n",
    "\n",
    "get give go\n",
    "\n",
    "had has have he hence her here hereafter hereby herein hereupon hers herself\n",
    "him himself his how however hundred\n",
    "\n",
    "i if in indeed into is it its itself\n",
    "\n",
    "keep\n",
    "\n",
    "last latter latterly least less\n",
    "\n",
    "just\n",
    "\n",
    "made make many may me meanwhile might mine more moreover most mostly move much\n",
    "must my myself\n",
    "\n",
    "name namely neither never nevertheless next nine no nobody none noone nor not\n",
    "nothing now nowhere\n",
    "\n",
    "of off often on once one only onto or other others otherwise our ours ourselves\n",
    "out over own\n",
    "\n",
    "part per perhaps please put\n",
    "\n",
    "quite\n",
    "\n",
    "rather re really regarding\n",
    "\n",
    "same say see seem seemed seeming seems serious several she should show side\n",
    "since six sixty so some somehow someone something sometime sometimes somewhere\n",
    "still such\n",
    "\n",
    "take ten than that the their them themselves then thence there thereafter\n",
    "thereby therefore therein thereupon these they third this those though three\n",
    "through throughout thru thus to together too top toward towards twelve twenty\n",
    "two\n",
    "\n",
    "under until up unless upon us used using\n",
    "\n",
    "various very very via was we well were what whatever when whence whenever where\n",
    "whereafter whereas whereby wherein whereupon wherever whether which while\n",
    "whither who whoever whole whom whose why will with within without would\n",
    "\n",
    "yet you your yours yourself yourselves\n",
    "\"\"\".split()\n",
    ")\n",
    "\n",
    "contractions = [\"n't\", \"'d\", \"'ll\", \"'m\", \"'re\", \"'s\", \"'ve\"]\n",
    "STOP_WORDS.update(contractions)\n",
    "\n",
    "for apostrophe in [\"‘\", \"’\"]:\n",
    "    for stopword in contractions:\n",
    "        STOP_WORDS.add(stopword.replace(\"'\", apostrophe))\n",
    "\n",
    "obsoletisms = [\"art\", \"hath\", \"thou\", \"thee\", \"unto\"] # obsolete word forms\n",
    "STOP_WORDS.update(obsoletisms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убираем из списка слова, если они входят в список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_new = list()\n",
    "for aword in word_list:\n",
    "    if aword not in STOP_WORDS:\n",
    "        word_list_new.append(aword)\n",
    "\n",
    "# print(len(word_list))\n",
    "# print(len(word_list_new))\n",
    "word_list = word_list_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проходим по всему набору слов и вносим ранее не встреченные слова в словарь, для ранее встреченных увеличиваем value на единицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = dict() # empty dictionary for all words\n",
    "# putting all the words into dict:\n",
    "for aword in word_list:\n",
    "    if aword in word_dict: # if already in dict\n",
    "        word_dict[aword] += 1            \n",
    "    else:                  # if not in dict yet\n",
    "        word_dict[aword] = 1  \n",
    "                 \n",
    "# print(word_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Находим число уникальных слов (без критерия похожести; \"стоп-слова\" уже удалены и не учитываются)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of the unique words in the text: 170\n"
     ]
    }
   ],
   "source": [
    "key_number = len(word_dict.keys())\n",
    "print(f'The number of the unique words in the text: {key_number}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сортируем словарь по увеличению частоты встречаемости слова, извлекаем десять последних (то есть самых частых) слов.\n",
    "Если 11-е, 12-е и т.д. слова встречаются так же часто, как и десятое, выводим также и их, потому что по факту они все делят десятое место в списке наиболее частых слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent words:\n",
      "1. The word \"saith\" has the frequency 13\n",
      "2. The word \"said\" has the frequency 12\n",
      "3. The word \"jesus\" has the frequency 12\n",
      "4. The word \"god\" has the frequency 12\n",
      "5. The word \"john\" has the frequency 9\n",
      "6. The word \"saw\" has the frequency 7\n",
      "7. The word \"son\" has the frequency 6\n",
      "8. The word \"sent\" has the frequency 6\n",
      "9. The word \"man\" has the frequency 6\n",
      "10. The word \"light\" has the frequency 6\n"
     ]
    }
   ],
   "source": [
    "# sorting the list of the most frequent words:\n",
    "word_dict_sorted = sorted(word_dict.items(), key = lambda item: item[1])\n",
    "print('The most frequent words:')\n",
    "\n",
    "# extracting the most frequent words\n",
    "for i in range(10):\n",
    "    aword = word_dict_sorted[-i-1][0]\n",
    "    frequency = word_dict_sorted[-i-1][1]\n",
    "    print(f'{i+1}. The word \"{aword}\" has the frequency {frequency}')\n",
    "\n",
    "# extracting also the 11th, 12th etc, if the frequency is the same as for the 10th one:\n",
    "while word_dict_sorted[-i-2][1] == word_dict_sorted[-10][1]:\n",
    "    aword = word_dict_sorted[-i-2][0]\n",
    "    frequency = word_dict_sorted[-i-2][1]\n",
    "    print(f'{i}. The word \"{aword}\" has frequency {frequency}')\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, нами был проделан анализ англоязычного текста в отношении количества уникальных слов без объединения словоформ по критерию схожести.\n",
    "\n",
    "Выводы:\n",
    "1. При анализе числа уникальных слов в тексте следует удалять т. наз. \"стоп-слова\", то есть артикли, предлоги и прочие слова, не несущие смысловой нагрузки.\n",
    "2. Для исключения ситуации, когда слова в начале и середине предложения считаются за разные слова за счет отличия регистра первой буквы, следует привести слова перед анализом к одному регистру.\n",
    "3. В классическом англоязычном тексте 17 в. на ~1000 слов (4102 знака без учета пробелов) после удаления стоп-слов оказалось 170 уникальных слов (без дальнейшего объединения словоформ по критерию схожести), то есть число уникальных значащих слов примерно в пять раз меньше, чем общее число всех слов.\n",
    "4. Десять отобранных уникальных слов с максимальной частотностью дают хорошее представление о тематике текста."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
